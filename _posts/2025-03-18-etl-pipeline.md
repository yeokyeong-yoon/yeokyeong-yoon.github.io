---
layout: post
title: "ETL 파이프라인: Databricks와 PySpark를 활용한 데이터 처리 시스템"
date: 2025-03-17 12:00:00 +0900
categories: [개발, 데이터엔지니어링]
tags: [etl, databricks, pyspark, aws-s3, data-engineering]
mermaid: true
---

## 1. 개요

최근 수요 예측과 가격 최적화 모델을 위한 데이터 처리 시스템 개발을 담당하게 되었다. 초기에는 내부 팀과 외부 업체 1곳의 데이터만을 처리하는 소규모 시스템으로 시작하였으나, 향후 다수의 데이터 소스를 수용해야 할 것으로 예상되어 확장성 있는 시스템 설계가 필수적이었다.

현재 수집되는 데이터를 분석한 결과, 데이터 품질에 있어 여러 문제점이 발견되었다. 누락된 정보, ID 중복, 비정상적인 값과 미처리된 null 값 등이 주요 이슈였으며, 이러한 데이터 품질 문제 해결이 최우선 과제로 대두되었다.

시스템 설계 과정에서 상세한 설계 문서를 작성하고, Raw 데이터 통합을 위한 초기 프로토타입을 구현하였다. 이 과정에서 다양한 기술적 검토와 심도 있는 분석이 요구되었다. 현재 프로젝트가 진행 중이지만, 지금까지의 개발 경험과 인사이트를 공유하고자 했다.

### 핵심 요구사항

- **확장성**: 데이터 제공 업체가 늘어나도 유연하게 대응할 수 있는 구조
- **데이터 품질**: 믿을 수 있고 일관된 데이터 제공
- **자동화**: 사람이 직접 개입하는 과정을 최소화한 데이터 처리
- **표준화**: 다양한 형식의 데이터를 일관된 형태로 변환

### 기대효과

데이터 처리 시스템을 구축함으로써 다음과 같은 효과를 기대할 수 있다:

- **수동 데이터 수집 과정 자동화**: 작업 효율 향상 및 사람의 실수 감소
- **일관된 데이터 처리 규칙 적용**: 데이터 품질 향상
- **체계적인 데이터 검증**: 신뢰할 수 있는 데이터 확보
- **재현 가능한 데이터 처리**: 투명성 및 추적 가능성 확보
- **모델링 시간 단축**: 데이터 준비 시간을 줄여 실제 분석 모델 개발에 집중

## 2. 아키텍처 설계

### 2.1 핵심 구성요소

- **AWS S3 (데이터 저장소)**: 원본 데이터와 처리된 데이터를 저장하는 공간이다. 대용량 데이터를 안정적으로 저장하고 필요할 때 쉽게 접근할 수 있다.

- **Databricks (처리 환경)**: 대규모 데이터 처리와 머신러닝을 위한 플랫폼으로, 팀원들이 함께 코드를 작성하고 실행할 수 있는 환경을 제공한다.

- **PySpark (데이터 처리 도구)**: 여러 컴퓨터에 데이터 처리 작업을 분산하여 대용량 데이터를 빠르게 처리할 수 있게 해주는 Python 라이브러리이다.

- **Parquet (데이터 저장 형식)**: 데이터를 효율적으로 압축하고 빠르게 읽을 수 있는 파일 형식으로, 데이터의 구조 정보도 함께 저장하여 일관성을 유지한다.

### 2.2 데이터 흐름도

<div class="mermaid">
graph TD
    subgraph "데이터 소스"
        A1[업체 1<br>일일 데이터 파일] 
        A2[업체 2<br>일일 데이터 파일]
        A3[업체 N<br>일일 데이터 파일]
    end

    subgraph "수집 레이어"
        B1[변환기 1]
        B2[변환기 2] 
        B3[변환기 N]
        B4[업체별 설정 관리]
    end
    
    subgraph "공통 데이터 처리 과정"
        C1[데이터 도착 확인]
        C2[데이터 검증 & 품질 관리]
        C3[데이터 변환 & 표준화]
        C4[데이터 적재]
    end
    
    subgraph "Databricks 분산 처리"
        D1[컴퓨팅 자원 관리]
        D2[성능 최적화]
        D3[자원 할당]
        D4[오류 복구]
    end
    
    subgraph "품질 관리"
        E1[비정상 값 탐지]
        E2[중복 데이터 확인]
        E3[데이터 구조 검증]
        E4[데이터 누락 감지]
    end
    
    subgraph "데이터 활용 대상"
        G1[분석용 데이터 저장소]
        G2[머신러닝 모델 학습 데이터]
        G3[가격 최적화 시스템]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C1
    B3 --> C1
    B4 -.-> B1
    B4 -.-> B2
    B4 -.-> B3
    
    C1 --> C2 --> C3 --> C4
    
    C2 <--> E1
    C2 <--> E2
    C2 <--> E3
    C2 <--> E4
    
    D1 -.-> C1
    D1 -.-> C2
    D1 -.-> C3
    D1 -.-> C4
    D2 -.-> C1
    D2 -.-> C2
    D2 -.-> C3
    D2 -.-> C4
    D3 -.-> C1
    D3 -.-> C2
    D3 -.-> C3
    D3 -.-> C4
    D4 -.-> C1
    D4 -.-> C2
    D4 -.-> C3
    D4 -.-> C4
    
    C4 --> G1
    C4 --> G2
    C4 --> G3
</div>

데이터 흐름은 다음과 같은 단계로 이루어진다:

1. **데이터 수집**: 각 업체의 데이터가 매일 일정 시간에 AWS S3에 저장된다.
2. **데이터 불러오기**: 저장된 데이터를 Databricks 환경으로 가져온다.
3. **데이터 처리 및 변환**: PySpark를 사용해 데이터의 품질을 검증하고, 일관된 형식으로 변환한다.
4. **결과 저장 및 활용**: 처리된 데이터를 Parquet 형식으로 저장하고, 이를 가격 최적화 모델과 다양한 분석에 활용한다.

## 3. 데이터 처리 파이프라인

### 3.1 데이터 수집 전략

#### 소스 데이터 종류 및 형식

외부 업체에서 제공하는 데이터는 다음과 같은 형식으로 제공된다:

- **구조화된 데이터**: CSV, JSON 같은 일반적인 형식의 거래 데이터
- **파일 전송 방식**: 매일 정해진 시간에 안전한 파일 전송 방식으로 전달
- **데이터 종류**: 상품 가격, 재고 정보, 예약 현황, 할인 정보 등

#### 수집 주기 및 방법

- **수집 주기**: 하루에 한 번, 주로 새벽 시간대에 처리
- **업체별 데이터 변환기**: 각 업체의 데이터 형식에 맞는 맞춤형 변환 프로그램 개발
- **메타데이터 관리**: 파일 도착 시간, 처리 상태, 데이터 품질 점수 등의 부가 정보 관리

### 3.2 데이터 검증 프로세스

데이터 품질을 보장하기 위해 4단계로 구성된 체계적인 검증 과정을 구현한다:

<div class="mermaid">
graph TD
    subgraph "기본 데이터 검증"
        A1[빈 값 확인]
        A2[데이터 유형 확인]
        A3[값 범위 확인]
        A4[데이터 형식 확인<br>정규표현식]
    end

    subgraph "업종 특화 검증"
        B1[가격 합리성 확인]
        B2[재고 관리 논리 검증]
        B3[예약 충돌 확인]
        B4[시즌별 가격 변동 검증]
        B5[할인 규칙 적용 확인]
        B6[특별 날짜 패턴 확인]
    end

    subgraph "이상 패턴 감지"
        C1[통계적 이상값 감지]
        C2[시간에 따른 패턴 분석]
        C3[급격한 변동 감지]
        C4[계절 패턴 적용]
    end

    subgraph "문제 대응"
        D1[심각도 분류]
        D2[업체 알림]
        D3[내부팀 알림]
        D4[자동 수정 적용]
        D5[수동 검토 요청]
        D6[문제 추적 관리]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    A4 --> B1
    A1 --> B2
    A2 --> B2
    A3 --> B2
    A4 --> B2
    A1 --> B3
    A2 --> B3
    A3 --> B3
    A4 --> B3
    A1 --> B4
    A2 --> B4
    A3 --> B4
    A4 --> B4
    A1 --> B5
    A2 --> B5
    A3 --> B5
    A4 --> B5
    A1 --> B6
    A2 --> B6
    A3 --> B6
    A4 --> B6
    
    B1 --> C1
    B2 --> C1
    B3 --> C1
    B4 --> C1
    B5 --> C1
    B6 --> C1
    B1 --> C2
    B2 --> C2
    B3 --> C2
    B4 --> C2
    B5 --> C2
    B6 --> C2
    B1 --> C3
    B2 --> C3
    B3 --> C3
    B4 --> C3
    B5 --> C3
    B6 --> C3
    B1 --> C4
    B2 --> C4
    B3 --> C4
    B4 --> C4
    B5 --> C4
    B6 --> C4
    
    C1 --> D1
    C2 --> D1
    C3 --> D1
    C4 --> D1
    
    D1 --> D2
    D1 --> D3
    D1 --> D4
    D1 --> D5
    
    D4 --> D6
    D5 --> D6
</div>

#### 검증 실패 시 대응 방안

데이터 검증에 실패했을 때는 문제의 심각도에 따라 다음과 같이 대응한다:

1. **매우 심각**: 데이터 처리 중단, 담당자에게 즉시 알림
2. **심각**: 처리는 계속하되 결과에 문제 표시, 담당자에게 알림
3. **중간**: 미리 정의된 규칙에 따라 자동 수정, 처리 기록 남김
4. **경미**: 경고 메시지 기록, 정기 보고서에 포함

### 3.3 데이터 변환 및 처리

#### PySpark 변환 로직

주요 데이터 처리 로직의 예시이다:

```python
# 주중/주말 가격 패턴 검증 예시
def validate_pricing_pattern(df):
    # 요일 정보 추출
    df['day_of_week'] = df['date'].dt.dayofweek
    
    # 주중/주말 구분
    weekday_prices = df[df['day_of_week'].isin([0,1,2,3,4])]['price']
    weekend_prices = df[df['day_of_week'].isin([5,6])]['price']
    
    # 평균 가격 계산
    avg_weekday = weekday_prices.mean()
    avg_weekend = weekend_prices.mean()
    
    # 일반적인 패턴 검증
    if avg_weekend < avg_weekday * 0.8:
        raise_alert(
            severity="High",
            message=f"비정상 가격 패턴 감지: 주말 평균 {avg_weekend}이 주중 평균 {avg_weekday}의 80% 미만",
            metrics={"weekend_avg": avg_weekend, "weekday_avg": avg_weekday, "ratio": avg_weekend/avg_weekday}
        )
```

#### 배치 처리 전략

- **계층적 데이터 관리**: 원본(Bronze), 검증(Silver), 집계(Gold) 단계로 데이터를 구분하여 관리
- **증분 처리**: 매일 새로운 데이터만 처리하여 작업 효율 높임
- **데이터 분할**: 날짜와 업체 ID를 기준으로 데이터를 나누어 검색 성능 향상

#### 성능 최적화 방안

- **중간 결과 저장**: 자주 사용하는 중간 계산 결과를 임시 저장하여 반복 계산 방지
- **병렬 처리**: 업체별로 독립적인 병렬 처리로 속도 향상
- **데이터 이동 최소화**: 데이터 이동이 적은 효율적인 결합 방법 선택
- **처리 단위 최적화**: 적절한 데이터 분할 단위 설정으로 분산 처리 효율 향상

## 4. 개발 및 운영 전략

### 4.1 개발 환경

#### Databricks 노트북 활용 방안

- **통합 개발 환경**: 코드, 설명, 시각화를 한 곳에서 작성하고 확인
- **변수 설정**: 실행 시 필요한 값을 쉽게 변경할 수 있는 방식으로 구성
- **함께 작업**: 여러 팀원이 동시에 같은 문서를 편집하고 의견 교환
- **변경 이력 관리**: 노트북의 모든 변경사항을 기록하고 필요시 이전 버전으로 복원

#### 버전 관리 및 협업

- **코드 관리 시스템 연동**: Databricks와 Git을 연결하여 코드 변경 이력 관리
- **환경 분리**: 개발, 테스트, 운영 환경을 분리하여 안정성 확보
- **코드 검토**: 변경 사항에 대한 팀원 검토 과정 도입
- **상세한 설명**: 코드 내 주석과 마크다운을 활용한 풍부한 문서화

### 4.2 배포 및 실행 일정 관리

#### Databricks 작업 활용

- **작업 흐름 정의**: 여러 단계의 작업을 순서대로 실행하는 흐름 구성
- **의존성 관리**: 선행 작업 완료 후 다음 작업이 실행되도록 설정
- **실패 대응**: 작업 실패 시 자동으로 다시 시도하는 기능 설정
- **자원 할당**: 각 작업에 맞는 최적의 컴퓨팅 자원 할당

#### 작업 일정 및 모니터링

- **일일 처리 시간**: 매일 새벽 2시에 자동으로 실행
- **조건부 실행**: 데이터가 도착했는지 확인 후 처리 시작
- **실행 기록**: 모든 실행 과정과 결과를 저장하고 분석
- **완료 시간 관리**: 작업이 예상 시간 내에 완료되는지 확인하고 지연 시 알림

### 4.3 모니터링 및 알림

#### 핵심 모니터링 지표

- **데이터 품질 지표**: 완전성(누락 없음), 정확성, 일관성, 적시성 등
- **시스템 성능 지표**: 처리 소요 시간, 컴퓨팅 자원 사용량, 오류 발생 비율
- **비즈니스 지표**: 데이터 양, 패턴 변화, 비정상 값 비율 등

#### 알림 설정 및 대응 프로세스

- **알림 방법**: Slack, 이메일 등을 통한 문제 상황 전파
- **담당자 지정**: 문제의 종류와 심각도에 따른 담당자 자동 지정
- **단계적 확대**: 지정 시간 내 응답이 없으면 상위 담당자에게 알림
- **문제 해결 기록**: 문제 발생부터 해결까지의 전체 과정 문서화

## 5. 확장성 및 향후 계획

### 데이터 규모 확장 대응 방안

- **자동 자원 확장**: 데이터 양에 따라 컴퓨팅 자원을 자동으로 늘리거나 줄임
- **데이터 분할 전략 개선**: 늘어나는 데이터에 맞게 효율적인 분할 방법 적용
- **저장 효율화**: 데이터 압축과 인덱싱으로 저장 공간 절약 및 검색 속도 향상

### 추가 데이터 소스 통합 가능성

- **표준화된 연결 과정**: 새로운 데이터 제공 업체를 쉽게 추가할 수 있는 절차 수립
- **데이터 구조 변화 관리**: 시간에 따라 변하는 데이터 구조에 대응하는 방법
- **다양한 형식 지원 확대**: API, 실시간 데이터 등 새로운 형태의 데이터 소스 통합 계획

### 실시간 처리 도입 검토

- **혼합 처리 방식**: 일괄 처리와 실시간 처리를 함께 사용하는 방식
- **이중 구조 아키텍처**: 느린 일괄 처리와 빠른 실시간 처리 레이어 조합
- **즉각적인 품질 관리**: 실시간으로 데이터 품질 문제를 감지하고 대응

## 6. 개인적인 소감 및 배운 점

### 파이프라인 구축과 데이터 품질의 중요성

파이프라인을 만드는 것이 주요 작업일 것이라고 생각했는데, 실제로는 데이터 품질 관리(DQA)가 훨씬 더 큰 작업이라는 점이 놀라웠다. 코드 몇 줄로 데이터를 A에서 B로 옮기는 것은 상대적으로 쉽지만, 그 데이터가 신뢰할 수 있고 일관된 것인지 확인하는 과정에 더 많은 노력이 필요했다. 특히 업종에 대한 지식을 활용한 검증 규칙을 만드는 것은 단순한 기술적 문제를 넘어, 비즈니스 규칙에 대한 깊은 이해가 필요한 작업이었다.

### 확장성과 과도한 설계 사이의 균형

처음에는 아직 존재하지 않는 데이터 소스들까지 고려하며 설계하려고 했다. 코드 수정을 최소화하기 위해 각 데이터 소스별로 어댑터를 만드는 방식을 선택했는데, 오히려 코드가 점점 복잡해지는 문제가 발생했다. 이 과정에서 깨달은 점은 확실히 추가될 기능이라면 미리 구현하되, 불확실한 미래를 위한 과도한 설계는 오히려 독이 될 수 있다는 것이었다. 결국 실용성과 확장성 사이에서 적절한 균형을 찾는 것이 가장 중요하다는 교훈을 얻었다.

### 실무자 협업의 중요성

이 프로젝트를 설계하고 기획하는 데 많은 회의와 다양한 팀원들과의 소통이 필요했다. 세상에 완벽한 데이터는 많지 않고, 각 데이터 제공자마다 데이터에 대한 이해와 관리 수준이 다르다는 점을 경험했다. 기술적인 해결책만으로는 충분하지 않으며, 효과적인 소통과 비즈니스 과정에 대한 이해가 성공적인 데이터 파이프라인 구축의 핵심이라는 점을 배웠다.

### 가격 최적화 모델에 대한 이해 확장

아직 가격 최적화 시스템의 전체적인 구조를 완전히 파악하지 못했지만, 이 프로젝트를 진행하면서 점차 더 깊이 이해할 수 있기를 기대한다. 데이터 파이프라인은 결국 분석 모델을 위한 기반이므로, 이 두 요소가 어떻게 유기적으로 연결되는지 더 명확히 파악하는 것이 다음 과제라고 생각한다. 데이터 품질이 모델 품질에 직결된다는 점을 이해하고, 파이프라인의 각 단계가 최종 목표에 어떻게 기여하는지 더 깊이 고민해 볼 것이다.

## 맺음말

데이터 처리 시스템 구축은 단순한 기술적 과제가 아니라 비즈니스 성공을 위한 핵심 기반이다. 확장 가능하고 안정적인 시스템을 통해 고품질 데이터를 확보함으로써, 분석 모델의 정확도를 높이고 다양한 데이터 출처 통합을 지원할 수 있다.

처음에는 생소했던 ETL과 데이터 품질 관리(DQA) 개념이 이제는 조금씩 명확해지고 있다. 앞으로 이 프로젝트를 통해 배울 것들이 많지만, 중요한 데이터를 처리하는 시스템을 설계하고 구축한다는 책임감과 설렘을 안고 도전해 나가겠다.

## 용어 설명

- **AWS S3(Simple Storage Service)**: 아마존에서 제공하는 클라우드 스토리지 서비스로, 무제한에 가까운 저장 공간과 높은 내구성을 제공한다.

- **Data Lake**: 원시 형태의 다양한 데이터를 저장하는 중앙 저장소로, 구조화/비구조화 데이터를 모두 보관할 수 있다.

- **Databricks**: Apache Spark 기반의 클라우드 기반 빅데이터 및 머신러닝 플랫폼으로, 데이터 분석, 처리, 모델링을 통합 환경에서 제공한다.

- **DQA(Data Quality Assurance)**: 데이터의 정확성, 완전성, 일관성 등을 보장하기 위한 품질 관리 활동이다.

- **ETL(Extract, Transform, Load)**: 데이터를 원천 시스템에서 추출하여, 변환 과정을 거친 후, 목적지 시스템에 적재하는 과정을 의미한다.

- **Lambda 아키텍처**: 배치 처리와 실시간 처리를 결합한 데이터 처리 아키텍처로, 지연 시간과 처리량 사이의 균형을 맞추는 방식이다.

- **Parquet**: 컬럼 기반 저장 형식으로, 데이터 압축 효율과 쿼리 성능이 뛰어난 오픈소스 파일 포맷이다.

- **PySpark**: Apache Spark의 Python API로, 대규모 데이터 처리를 위한 분산 컴퓨팅 프레임워크이다.

- **SLA(Service Level Agreement)**: 서비스 제공자와 사용자 간에 합의된 서비스 수준(가용성, 성능 등)을 명시한 계약이다.

- **어댑터 패턴(Adapter Pattern)**: 서로 다른 인터페이스를 가진 클래스들이 함께 작동할 수 있도록 중간에서 변환해주는 설계 패턴이다.

- **오버엔지니어링(Over-engineering)**: 실제 필요 이상으로 복잡하게 시스템을 설계하는 것으로, 유지보수 비용을 증가시킬 수 있다.

- **증분 처리(Incremental Processing)**: 전체 데이터가 아닌 새로 추가된 데이터만 처리하는 방식으로, 효율성을 높인다.

- **파티셔닝(Partitioning)**: 데이터를 특정 기준(날짜, 지역 등)으로 나누어 저장하는 기법으로, 쿼리 성능을 향상시킨다.
