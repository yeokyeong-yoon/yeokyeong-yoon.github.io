---
layout: post
title: "ETL 파이프라인: Databricks와 PySpark를 활용한 데이터 처리 시스템"
date: 2025-03-17 12:00:00 +0900
categories: [개발, 데이터엔지니어링]
tags: [etl, databricks, pyspark, aws-s3, data-engineering]
mermaid: true
---

<style>
.mermaid {
  width: 100%;
  max-width: 1200px;
  margin: 40px auto;
  font-size: 18px;
  font-family: 'Arial', sans-serif;
  overflow: visible;
}
.mermaid .node rect, 
.mermaid .node circle, 
.mermaid .node ellipse, 
.mermaid .node polygon, 
.mermaid .node path {
  fill: #f5f9ff;
  stroke: #4a6da7;
  stroke-width: 1.5px;
}
.mermaid .node text {
  font-size: 18px;
  font-weight: 500;
}
.mermaid .edgeLabel {
  font-size: 16px;
  background-color: white;
  padding: 4px 8px;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}
.mermaid .cluster rect {
  fill: #f0f8ff;
  stroke: #4a6da7;
  stroke-width: 1px;
  rx: 8px;
  ry: 8px;
}
.mermaid .label {
  font-size: 20px;
  font-weight: bold;
}
.mermaid .timeline-event {
  font-size: 16px;
}
.mermaid .journey-section {
  font-size: 18px;
  font-weight: bold;
}
</style>

## 1. 개요

최근 수요 예측과 가격 최적화 모델을 위한 데이터 처리 시스템 개발을 담당하게 되었다. 초기에는 내부 팀과 외부 업체 1곳의 데이터만을 처리하는 소규모 시스템으로 시작하였으나, 향후 다수의 데이터 소스를 수용해야 할 것으로 예상되어 확장성 있는 시스템 설계가 필수적이었다.

현재 수집되는 데이터를 분석한 결과, 데이터 품질에 있어 여러 문제점이 발견되었다. 누락된 정보, ID 중복, 비정상적인 값과 미처리된 null 값 등이 주요 이슈였으며, 이러한 데이터 품질 문제 해결이 최우선 과제로 대두되었다.

시스템 설계 과정에서 상세한 설계 문서를 작성하고, Raw 데이터 통합을 위한 초기 프로토타입을 구현하였다. 이 과정에서 다양한 기술적 검토와 심도 있는 분석이 요구되었다. 현재 프로젝트가 진행 중이지만, 지금까지의 개발 경험과 인사이트를 공유하고자 했다.

### 핵심 요구사항

- **확장성**: 데이터 제공 업체가 늘어나도 유연하게 대응할 수 있는 구조
- **데이터 품질**: 믿을 수 있고 일관된 데이터 제공
- **자동화**: 사람이 직접 개입하는 과정을 최소화한 데이터 처리
- **표준화**: 다양한 형식의 데이터를 일관된 형태로 변환

### 기대효과

데이터 처리 시스템을 구축함으로써 다음과 같은 효과를 기대할 수 있다:

- **수동 데이터 수집 과정 자동화**: 작업 효율 향상 및 사람의 실수 감소
- **일관된 데이터 처리 규칙 적용**: 데이터 품질 향상
- **체계적인 데이터 검증**: 신뢰할 수 있는 데이터 확보
- **재현 가능한 데이터 처리**: 투명성 및 추적 가능성 확보
- **모델링 시간 단축**: 데이터 준비 시간을 줄여 실제 분석 모델 개발에 집중

## 2. 아키텍처 설계

### 2.1 핵심 구성요소

- **AWS S3 (데이터 저장소)**: 원본 데이터와 처리된 데이터를 저장하는 공간이다. 대용량 데이터를 안정적으로 저장하고 필요할 때 쉽게 접근할 수 있다.

- **Databricks (처리 환경)**: 대규모 데이터 처리와 머신러닝을 위한 플랫폼으로, 팀원들이 함께 코드를 작성하고 실행할 수 있는 환경을 제공한다.

- **PySpark (데이터 처리 도구)**: 여러 컴퓨터에 데이터 처리 작업을 분산하여 대용량 데이터를 빠르게 처리할 수 있게 해주는 Python 라이브러리이다.

- **Parquet (데이터 저장 형식)**: 데이터를 효율적으로 압축하고 빠르게 읽을 수 있는 파일 형식으로, 데이터의 구조 정보도 함께 저장하여 일관성을 유지한다.

<div class="mermaid">
flowchart LR
    classDef storage fill:#f9f7ed,stroke:#d4b483,stroke-width:2px
    classDef process fill:#e1f0fa,stroke:#4a6fa5,stroke-width:2px
    classDef data fill:#ebf5ee,stroke:#58a4b0,stroke-width:2px
    
    S3["AWS S3<br><small>데이터 저장소</small>"]:::storage
    DB["Databricks<br><small>처리 환경</small>"]:::process
    PS["PySpark<br><small>데이터 처리</small>"]:::process
    ML["ML 모델<br><small>가격 최적화</small>"]:::data
    
    S3 --> |"원본 데이터"| DB
    DB --> |"처리"| PS
    PS --> |"변환된 데이터"| S3
    S3 --> |"학습 데이터"| ML
    
    style S3 font-size:18px,font-weight:bold
    style DB font-size:18px,font-weight:bold
    style PS font-size:18px,font-weight:bold
    style ML font-size:18px,font-weight:bold
</div>

*이 플로우차트는 ETL 파이프라인의 주요 구성 요소(AWS S3, Databricks, PySpark, ML 모델) 간의 데이터 흐름을 보여줍니다. 각 구성 요소는 서로 다른 색상으로 구분되어 역할을 명확히 합니다.*

### 2.2 데이터 흐름도

<div class="mermaid">
timeline
    title ETL 파이프라인 처리 흐름
    section 데이터 수집
        원본 데이터 도착 : 업체 파일 수신
        초기 점검 : 파일 형식 및 구조 검증
    section 데이터 처리
        변환 : 표준 형식으로 변환
        검증 : 품질 검사 적용
        보강 : 파생 속성 추가
    section 데이터 저장
        저장소 적재 : Parquet 파일로 저장
        분할 : 날짜 및 출처별 구성
    section 데이터 활용
        분석 : 분석 쿼리 지원
        ML 모델 : 가격 최적화 모델 학습
</div>

*이 타임라인 다이어그램은 ETL 과정의 주요 단계(데이터 수집, 처리, 저장, 활용)를 시간 순서에 따라 표현합니다.*

데이터 흐름은 다음과 같은 단계로 이루어진다:

1. **데이터 수집**: 각 업체의 데이터가 매일 일정 시간에 AWS S3에 저장된다.
2. **데이터 불러오기**: 저장된 데이터를 Databricks 환경으로 가져온다.
3. **데이터 처리 및 변환**: PySpark를 사용해 데이터의 품질을 검증하고, 일관된 형식으로 변환한다.
4. **결과 저장 및 활용**: 처리된 데이터를 Parquet 형식으로 저장하고, 이를 가격 최적화 모델과 다양한 분석에 활용한다.

## 3. 데이터 처리 파이프라인

### 3.1 데이터 수집 전략

#### 소스 데이터 종류 및 형식

외부 업체에서 제공하는 데이터는 다음과 같은 형식으로 제공된다:

- **구조화된 데이터**: CSV, JSON 같은 일반적인 형식의 거래 데이터
- **파일 전송 방식**: 매일 정해진 시간에 안전한 파일 전송 방식으로 전달
- **데이터 종류**: 상품 가격, 재고 정보, 예약 현황, 할인 정보 등

#### 수집 주기 및 방법

- **수집 주기**: 하루에 한 번, 주로 새벽 시간대에 처리
- **업체별 데이터 변환기**: 각 업체의 데이터 형식에 맞는 맞춤형 변환 프로그램 개발
- **메타데이터 관리**: 파일 도착 시간, 처리 상태, 데이터 품질 점수 등의 부가 정보 관리

### 3.2 데이터 검증 프로세스

데이터 품질을 보장하기 위해 4단계로 구성된 체계적인 검증 과정을 구현한다:

<div class="mermaid">
stateDiagram-v2
    [*] --> 기본검증
    
    state 기본검증 {
        빈값확인
        데이터유형확인
        값범위확인
        형식확인
    }
    
    기본검증 --> 업종특화검증
    
    state 업종특화검증 {
        가격합리성확인
        재고관리검증
        예약충돌확인
        시즌별가격검증
        할인규칙확인
        날짜패턴확인
    }
    
    업종특화검증 --> 이상패턴감지
    
    state 이상패턴감지 {
        통계적이상감지
        시간패턴분석
        급격한변동감지
        계절패턴적용
    }
    
    이상패턴감지 --> 문제대응
    
    state 문제대응 {
        심각도분류
        업체알림
        내부팀알림
        자동수정적용
        수동검토요청
        문제추적관리
    }
    
    문제대응 --> [*]
</div>

*이 상태 다이어그램은 데이터 검증 프로세스의 4단계(기본 검증, 업종 특화 검증, 이상 패턴 감지, 문제 대응)와 각 단계 내의 세부 작업을 보여줍니다.*

#### 검증 실패 시 대응 방안

데이터 검증에 실패했을 때는 문제의 심각도에 따라 다음과 같이 대응한다:

1. **매우 심각**: 데이터 처리 중단, 담당자에게 즉시 알림
2. **심각**: 처리는 계속하되 결과에 문제 표시, 담당자에게 알림
3. **중간**: 미리 정의된 규칙에 따라 자동 수정, 처리 기록 남김
4. **경미**: 경고 메시지 기록, 정기 보고서에 포함

### 3.3 데이터 변환 및 처리

#### PySpark 변환 로직

주요 데이터 처리 로직의 예시이다:

```python
# 주중/주말 가격 패턴 검증 예시
def validate_pricing_pattern(df):
    # 요일 정보 추출
    df['day_of_week'] = df['date'].dt.dayofweek
    
    # 주중/주말 구분
    weekday_prices = df[df['day_of_week'].isin([0,1,2,3,4])]['price']
    weekend_prices = df[df['day_of_week'].isin([5,6])]['price']
    
    # 평균 가격 계산
    avg_weekday = weekday_prices.mean()
    avg_weekend = weekend_prices.mean()
    
    # 일반적인 패턴 검증
    if avg_weekend < avg_weekday * 0.8:
        raise_alert(
            severity="High",
            message=f"비정상 가격 패턴 감지: 주말 평균 {avg_weekend}이 주중 평균 {avg_weekday}의 80% 미만",
            metrics={"weekend_avg": avg_weekend, "weekday_avg": avg_weekday, "ratio": avg_weekend/avg_weekday}
        )
```

#### 배치 처리 전략

- **계층적 데이터 관리**: 원본(Bronze), 검증(Silver), 집계(Gold) 단계로 데이터를 구분하여 관리
- **증분 처리**: 매일 새로운 데이터만 처리하여 작업 효율 높임
- **데이터 분할**: 날짜와 업체 ID를 기준으로 데이터를 나누어 검색 성능 향상

#### 성능 최적화 방안

- **중간 결과 저장**: 자주 사용하는 중간 계산 결과를 임시 저장하여 반복 계산 방지
- **병렬 처리**: 업체별로 독립적인 병렬 처리로 속도 향상
- **데이터 이동 최소화**: 데이터 이동이 적은 효율적인 결합 방법 선택
- **처리 단위 최적화**: 적절한 데이터 분할 단위 설정으로 분산 처리 효율 향상

## 4. 개발 및 운영 전략

### 4.1 개발 환경

#### Databricks 노트북 활용 방안

- **통합 개발 환경**: 코드, 설명, 시각화를 한 곳에서 작성하고 확인
- **변수 설정**: 실행 시 필요한 값을 쉽게 변경할 수 있는 방식으로 구성
- **함께 작업**: 여러 팀원이 동시에 같은 문서를 편집하고 의견 교환
- **변경 이력 관리**: 노트북의 모든 변경사항을 기록하고 필요시 이전 버전으로 복원

#### 버전 관리 및 협업

- **코드 관리 시스템 연동**: Databricks와 Git을 연결하여 코드 변경 이력 관리
- **환경 분리**: 개발, 테스트, 운영 환경을 분리하여 안정성 확보
- **코드 검토**: 변경 사항에 대한 팀원 검토 과정 도입
- **상세한 설명**: 코드 내 주석과 마크다운을 활용한 풍부한 문서화

### 4.2 ETL 파이프라인 운영 프로세스

<div class="mermaid">
flowchart LR
    classDef extract fill:#f9d5e5,stroke:#d64161,stroke-width:2px
    classDef transform fill:#dcf2e9,stroke:#20b2aa,stroke-width:2px
    classDef load fill:#e6e6fa,stroke:#6a5acd,stroke-width:2px
    
    subgraph 추출단계["추출(Extract)"]
        E1["데이터 도착 확인<br><small>파일 존재 여부 체크</small><br><small>기본 메타데이터 확인</small>"]:::extract
        E2["데이터 수집<br><small>Raw 데이터 적재</small><br><small>수집 이력 기록</small>"]:::extract
    end
    
    subgraph 변환단계["변환(Transform)"]
        T1["기본 검증<br><small>필수 컬럼 확인</small><br><small>데이터 타입 검증</small>"]:::transform
        T2["업종별 검증<br><small>업종 코드 검증</small><br><small>필수값 존재 확인</small>"]:::transform
        T3["표준화<br><small>공통 스키마 적용</small><br><small>코드값 정규화</small>"]:::transform
    end
    
    subgraph 적재단계["적재(Load)"]
        L1["데이터 적재<br><small>적재 위치 확인</small><br><small>적재 이력 기록</small>"]:::load
        L2["적재 후 검증<br><small>데이터 건수 확인</small><br><small>무결성 체크</small>"]:::load
    end
    
    E1 --> E2 --> T1 --> T2 --> T3 --> L1 --> L2
    
    style 추출단계 fill:#fef6f8,stroke:#d64161,stroke-width:2px
    style 변환단계 fill:#f0f9f6,stroke:#20b2aa,stroke-width:2px
    style 적재단계 fill:#f5f5fd,stroke:#6a5acd,stroke-width:2px
</div>

*이 플로우차트는 ETL 과정의 각 단계별 작업, 중요도(1-5점 척도), 그리고 담당 시스템이나 팀을 보여줍니다. 추출, 변환, 적재 단계가 색상으로 구분되어 있습니다.*

#### ETL 파이프라인 운영 단계 설명

ETL 파이프라인의 운영은 크게 세 단계로 나뉘며, 각 단계별로 다음과 같은 주요 작업들이 있습니다:

1. **추출(Extract) 단계**:
   - **데이터 도착 확인**: 외부 업체로부터 데이터가 정해진 시간에 도착했는지 확인합니다. ETL 시스템이 자동으로 수행하며, 중요도가 매우 높습니다(5점).
   - **데이터 수집**: S3 버킷에서 Databricks 환경으로 데이터를 가져오는 작업입니다. 자동화된 프로세스로 수행되며 중요도가 높습니다(5점).

2. **변환(Transform) 단계**:
   - **기본 검증**: 데이터 형식, 범위, 필수값 존재 여부 등을 확인합니다. 자동화된 검증 프로세스를 통해 수행되며, 난이도는 보통 수준입니다(3점).
   - **업종별 검증**: 비즈니스 규칙에 맞는 데이터인지 검증합니다. 자동화된 규칙 기반 검증이 수행되며, 난이도가 높은 편입니다(4점).
   - **표준화**: 다양한 형식의 데이터를 일관된 형태로 변환합니다. 정의된 표준화 규칙에 따라 자동으로 처리되며 중요도가 매우 높습니다(5점).

3. **적재(Load) 단계**:
   - **데이터 적재**: 변환된 데이터를 Parquet 형식으로 S3에 저장합니다. 자동화된 프로세스로 수행되며 중요도가 높습니다(5점).
   - **적재 후 검증**: 저장된 데이터의 정합성을 최종 확인합니다. 자동화된 검증 프로세스를 통해 수행되며 난이도가 높습니다(4점).

이러한 단계적 접근을 통해 데이터 처리의 신뢰성과 효율성을 확보할 수 있습니다. 각 단계는 자동화된 시스템을 통해 처리되며, 문제 발생 시 즉각적인 알림과 로깅을 통해 신속한 대응이 가능하도록 설계되어 있습니다.

### 4.3 개발 로드맵

<div class="mermaid">
gantt
    title ETL 파이프라인 개발 로드맵
    dateFormat  YYYY-MM-DD
    axisFormat %y-%m
    
    section 완료된 작업
    데이터 수집 자동화      :done, task1, 2025-02-01, 2025-03-15
    기본 ETL 파이프라인 구축 :done, task2, 2025-02-15, 2025-04-15
    
    section 진행중
    데이터 품질 관리 체계화  :active, task3, 2025-04-01, 2025-05-31
    
    section 단기 계획 (6개월)
    추가 데이터 소스 통합    :task4, 2025-06-01, 90d
    성능 최적화            :task5, after task4, 60d
    모니터링 개선           :task6, 2025-09-01, 90d
    
    section 장기 계획
    ML 파이프라인 연동      :task8, 2025-10-01, 90d
</div>

*이 간트 차트는 ETL 파이프라인 개발의 타임라인을 보여줍니다. 2025년 2월에 시작된 프로젝트의 완료된 작업(파란색), 진행 중인 작업(노란색), 그리고 계획된 작업이 시간순으로 정렬되어 있습니다.*

## 5. 확장성 및 향후 계획

### 데이터 규모 확장 대응 방안

- **자동 자원 확장**: 데이터 양에 따라 컴퓨팅 자원을 자동으로 늘리거나 줄임
- **데이터 분할 전략 개선**: 늘어나는 데이터에 맞게 효율적인 분할 방법 적용
- **저장 효율화**: 데이터 압축과 인덱싱으로 저장 공간 절약 및 검색 속도 향상

### 추가 데이터 소스 통합 가능성

- **표준화된 연결 과정**: 새로운 데이터 제공 업체를 쉽게 추가할 수 있는 절차 수립
- **데이터 구조 변화 관리**: 시간에 따라 변하는 데이터 구조에 대응하는 방법
- **다양한 형식 지원 확대**: API, 실시간 데이터 등 새로운 형태의 데이터 소스 통합 계획

## 6. 개인적인 소감 및 배운 점

### 파이프라인 구축과 데이터 품질의 중요성

파이프라인을 만드는 것이 주요 작업일 것이라고 생각했는데, 실제로는 데이터 품질 관리(DQA)가 훨씬 더 큰 작업이라는 점이 놀라웠다. 코드 몇 줄로 데이터를 A에서 B로 옮기는 것은 상대적으로 쉽지만, 그 데이터가 신뢰할 수 있고 일관된 것인지 확인하는 과정에 더 많은 노력이 필요했다. 특히 업종에 대한 지식을 활용한 검증 규칙을 만드는 것은 단순한 기술적 문제를 넘어, 비즈니스 규칙에 대한 깊은 이해가 필요한 작업이었다.

### 확장성과 과도한 설계 사이의 균형

아직 추가되지도 않은 데이터 소스들을 미리 고려하면서 설계하는 것이 생각보다 어려웠다. 코드를 나중에 적게 수정할 수 있도록 만들려고 했는데, 오히려 각 데이터 출처별로 맞춤형 변환기를 만드는 방식을 선택하니 코드가 계속 늘어나는 것 같았다. 이런 접근법에는 장단점이 있다고 느꼈다. 확실히 추가될 요소라면 미리 구현하되, 불확실한 미래를 위해 너무 복잡한 설계를 하는 것은 과감히 포기하는 것도 필요하다는 교훈을 얻었다. 실용성과 확장성 사이에서 적절한 균형을 찾는 것이 매우 중요하다는 점을 배웠다.

### 실무자 협업의 중요성

이 프로젝트를 설계하고 기획하는 데 많은 회의와 다양한 팀원들과의 소통이 필요했다. 세상에 완벽한 데이터는 많지 않고, 각 데이터 제공자마다 데이터에 대한 이해와 관리 수준이 다르다는 점을 경험했다. 기술적인 해결책만으로는 충분하지 않으며, 효과적인 소통과 비즈니스 과정에 대한 이해가 성공적인 데이터 파이프라인 구축의 핵심이라는 점을 배웠다.

### 가격 최적화 모델에 대한 이해 확장

아직 가격 최적화 시스템의 전체적인 구조를 완전히 파악하지 못했지만, 이