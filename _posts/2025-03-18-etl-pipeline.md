---
layout: post
title: "ETL 파이프라인: Databricks와 PySpark를 활용한 데이터 처리 시스템"
date: 2025-03-17 12:00:00 +0900
categories: [개발, 데이터엔지니어링]
tags: [etl, databricks, pyspark, aws-s3, data-engineering]
mermaid: true
---

## 1. 개요

최근 수요 예측과 가격 최적화 모델을 위한 데이터 처리 시스템 개발을 담당하게 되었다. 초기에는 내부 팀과 외부 업체 1곳의 데이터만을 처리하는 소규모 시스템으로 시작하였으나, 향후 다수의 데이터 소스를 수용해야 할 것으로 예상되어 확장성 있는 시스템 설계가 필수적이었다.

현재 수집되는 데이터를 분석한 결과, 데이터 품질에 있어 여러 문제점이 발견되었다. 누락된 정보, ID 중복, 비정상적인 값과 미처리된 null 값 등이 주요 이슈였으며, 이러한 데이터 품질 문제 해결이 최우선 과제로 대두되었다.

시스템 설계 과정에서 상세한 설계 문서를 작성하고, Raw 데이터 통합을 위한 초기 프로토타입을 구현하였다. 이 과정에서 다양한 기술적 검토와 심도 있는 분석이 요구되었다. 현재 프로젝트가 진행 중이지만, 지금까지의 개발 경험과 인사이트를 공유하고자 했다.

### 핵심 요구사항

- **확장성**: 데이터 제공 업체가 늘어나도 유연하게 대응할 수 있는 구조
- **데이터 품질**: 믿을 수 있고 일관된 데이터 제공
- **자동화**: 사람이 직접 개입하는 과정을 최소화한 데이터 처리
- **표준화**: 다양한 형식의 데이터를 일관된 형태로 변환

### 기대효과

데이터 처리 시스템을 구축함으로써 다음과 같은 효과를 기대할 수 있다:

- **수동 데이터 수집 과정 자동화**: 작업 효율 향상 및 사람의 실수 감소
- **일관된 데이터 처리 규칙 적용**: 데이터 품질 향상
- **체계적인 데이터 검증**: 신뢰할 수 있는 데이터 확보
- **재현 가능한 데이터 처리**: 투명성 및 추적 가능성 확보
- **모델링 시간 단축**: 데이터 준비 시간을 줄여 실제 분석 모델 개발에 집중

## 2. 아키텍처 설계

### 2.1 핵심 구성요소

- **AWS S3 (데이터 저장소)**: 원본 데이터와 처리된 데이터를 저장하는 공간이다. 대용량 데이터를 안정적으로 저장하고 필요할 때 쉽게 접근할 수 있다.

- **Databricks (처리 환경)**: 대규모 데이터 처리와 머신러닝을 위한 플랫폼으로, 팀원들이 함께 코드를 작성하고 실행할 수 있는 환경을 제공한다.

- **PySpark (데이터 처리 도구)**: 여러 컴퓨터에 데이터 처리 작업을 분산하여 대용량 데이터를 빠르게 처리할 수 있게 해주는 Python 라이브러리이다.

- **Parquet (데이터 저장 형식)**: 데이터를 효율적으로 압축하고 빠르게 읽을 수 있는 파일 형식으로, 데이터의 구조 정보도 함께 저장하여 일관성을 유지한다.

<div class="mermaid">
C4Context
    title ETL 파이프라인 아키텍처 구성요소
    
    Container(s3, "AWS S3", "데이터 저장소", "원본 및 처리된 데이터 저장")
    Container(databricks, "Databricks", "처리 환경", "분산 컴퓨팅 플랫폼")
    Container(pyspark, "PySpark", "데이터 처리", "변환 및 검증 로직")
    Container(models, "ML 모델", "가격 최적화", "처리된 데이터 소비")
    
    Rel(s3, databricks, "원본 데이터")
    Rel(databricks, pyspark, "데이터 처리")
    Rel(pyspark, s3, "처리된 데이터")
    Rel(s3, models, "학습 데이터")
</div>

*이 다이어그램은 ETL 파이프라인의 주요 구성 요소(AWS S3, Databricks, PySpark, ML 모델)와 이들 간의 데이터 흐름을 보여줍니다.*

### 2.2 데이터 흐름도

<div class="mermaid">
timeline
    title ETL Pipeline Process Flow
    section Data Collection
        Raw data arrives : Provider files received
        Initial check : Validate file format and structure
    section Data Processing
        Transformation : Convert to standard format
        Validation : Apply quality checks
        Enrichment : Add derived attributes
    section Data Storage
        Load to storage : Save to Parquet files
        Partition : Organize by date and source
    section Data Consumption
        Analytics : Support analytics queries
        ML Models : Train price optimization models
</div>

*이 타임라인 다이어그램은 ETL 과정의 주요 단계(데이터 수집, 처리, 저장, 활용)를 시간 순서에 따라 표현합니다.*

데이터 흐름은 다음과 같은 단계로 이루어진다:

1. **데이터 수집**: 각 업체의 데이터가 매일 일정 시간에 AWS S3에 저장된다.
2. **데이터 불러오기**: 저장된 데이터를 Databricks 환경으로 가져온다.
3. **데이터 처리 및 변환**: PySpark를 사용해 데이터의 품질을 검증하고, 일관된 형식으로 변환한다.
4. **결과 저장 및 활용**: 처리된 데이터를 Parquet 형식으로 저장하고, 이를 가격 최적화 모델과 다양한 분석에 활용한다.

## 3. 데이터 처리 파이프라인

### 3.1 데이터 수집 전략

#### 소스 데이터 종류 및 형식

외부 업체에서 제공하는 데이터는 다음과 같은 형식으로 제공된다:

- **구조화된 데이터**: CSV, JSON 같은 일반적인 형식의 거래 데이터
- **파일 전송 방식**: 매일 정해진 시간에 안전한 파일 전송 방식으로 전달
- **데이터 종류**: 상품 가격, 재고 정보, 예약 현황, 할인 정보 등

#### 수집 주기 및 방법

- **수집 주기**: 하루에 한 번, 주로 새벽 시간대에 처리
- **업체별 데이터 변환기**: 각 업체의 데이터 형식에 맞는 맞춤형 변환 프로그램 개발
- **메타데이터 관리**: 파일 도착 시간, 처리 상태, 데이터 품질 점수 등의 부가 정보 관리

### 3.2 데이터 검증 프로세스

데이터 품질을 보장하기 위해 4단계로 구성된 체계적인 검증 과정을 구현한다:

<div class="mermaid">
stateDiagram-v2
    [*] --> BasicValidation
    
    state BasicValidation {
        EmptyCheck
        TypeCheck
        RangeCheck
        FormatCheck
    }
    
    BasicValidation --> DomainValidation
    
    state DomainValidation {
        PriceValidation
        InventoryValidation
        ReservationValidation
        SeasonalValidation
        DiscountValidation
        DatePatternValidation
    }
    
    DomainValidation --> AnomalyDetection
    
    state AnomalyDetection {
        StatisticalAnalysis
        TimePatternAnalysis
        SuddenChangeDetection
        SeasonalPatternApplication
    }
    
    AnomalyDetection --> ResponseAction
    
    state ResponseAction {
        SeverityClassification
        VendorNotification
        InternalTeamNotification
        AutomaticCorrection
        ManualReview
        IssueTracking
    }
    
    ResponseAction --> [*]
</div>

*이 상태 다이어그램은 데이터 검증 프로세스의 4단계(기본 검증, 업종 특화 검증, 이상 패턴 감지, 문제 대응)와 각 단계 내의 세부 작업을 보여줍니다.*

#### 검증 실패 시 대응 방안

데이터 검증에 실패했을 때는 문제의 심각도에 따라 다음과 같이 대응한다:

1. **매우 심각**: 데이터 처리 중단, 담당자에게 즉시 알림
2. **심각**: 처리는 계속하되 결과에 문제 표시, 담당자에게 알림
3. **중간**: 미리 정의된 규칙에 따라 자동 수정, 처리 기록 남김
4. **경미**: 경고 메시지 기록, 정기 보고서에 포함

### 3.3 데이터 변환 및 처리

#### PySpark 변환 로직

주요 데이터 처리 로직의 예시이다:

  ```python
# 주중/주말 가격 패턴 검증 예시
def validate_pricing_pattern(df):
      # 요일 정보 추출
      df['day_of_week'] = df['date'].dt.dayofweek
      
      # 주중/주말 구분
      weekday_prices = df[df['day_of_week'].isin([0,1,2,3,4])]['price']
      weekend_prices = df[df['day_of_week'].isin([5,6])]['price']
      
    # 평균 가격 계산
      avg_weekday = weekday_prices.mean()
      avg_weekend = weekend_prices.mean()
      
    # 일반적인 패턴 검증
      if avg_weekend < avg_weekday * 0.8:
          raise_alert(
              severity="High",
            message=f"비정상 가격 패턴 감지: 주말 평균 {avg_weekend}이 주중 평균 {avg_weekday}의 80% 미만",
              metrics={"weekend_avg": avg_weekend, "weekday_avg": avg_weekday, "ratio": avg_weekend/avg_weekday}
          )
  ```

#### 배치 처리 전략

- **계층적 데이터 관리**: 원본(Bronze), 검증(Silver), 집계(Gold) 단계로 데이터를 구분하여 관리
- **증분 처리**: 매일 새로운 데이터만 처리하여 작업 효율 높임
- **데이터 분할**: 날짜와 업체 ID를 기준으로 데이터를 나누어 검색 성능 향상

#### 성능 최적화 방안

- **중간 결과 저장**: 자주 사용하는 중간 계산 결과를 임시 저장하여 반복 계산 방지
- **병렬 처리**: 업체별로 독립적인 병렬 처리로 속도 향상
- **데이터 이동 최소화**: 데이터 이동이 적은 효율적인 결합 방법 선택
- **처리 단위 최적화**: 적절한 데이터 분할 단위 설정으로 분산 처리 효율 향상

## 4. 개발 및 운영 전략

<div class="mermaid">
journey
    title ETL 파이프라인 운영 여정
    section 추출(Extract)
        데이터 도착 확인: 5: ETL 시스템
        데이터 수집: 5: ETL 시스템
    section 변환(Transform)
        기본 검증: 3: ETL 시스템, 데이터 품질팀
        업종별 검증: 4: ETL 시스템, 비즈니스 분석가
        표준화: 5: ETL 시스템
    section 적재(Load)
        데이터 적재: 5: ETL 시스템
        적재 후 검증: 4: ETL 시스템, 데이터 품질팀
</div>

*이 여정 다이어그램은 ETL 과정의 각 단계별 작업과 해당 작업의 난이도(1-5점 척도), 그리고 담당 시스템이나 팀을 보여줍니다.*

### 4.1 개발 환경

#### Databricks 노트북 활용 방안

- **통합 개발 환경**: 코드, 설명, 시각화를 한 곳에서 작성하고 확인
- **변수 설정**: 실행 시 필요한 값을 쉽게 변경할 수 있는 방식으로 구성
- **함께 작업**: 여러 팀원이 동시에 같은 문서를 편집하고 의견 교환
- **변경 이력 관리**: 노트북의 모든 변경사항을 기록하고 필요시 이전 버전으로 복원

#### 버전 관리 및 협업

- **코드 관리 시스템 연동**: Databricks와 Git을 연결하여 코드 변경 이력 관리
- **환경 분리**: 개발, 테스트, 운영 환경을 분리하여 안정성 확보
- **코드 검토**: 변경 사항에 대한 팀원 검토 과정 도입
- **상세한 설명**: 코드 내 주석과 마크다운을 활용한 풍부한 문서화

### 4.2 배포 및 실행 일정 관리

#### Databricks 작업 활용

- **작업 흐름 정의**: 여러 단계의 작업을 순서대로 실행하는 흐름 구성
- **의존성 관리**: 선행 작업 완료 후 다음 작업이 실행되도록 설정
- **실패 대응**: 작업 실패 시 자동으로 다시 시도하는 기능 설정
- **자원 할당**: 각 작업에 맞는 최적의 컴퓨팅 자원 할당

#### 작업 일정 및 모니터링

- **일일 처리 시간**: 매일 새벽 2시에 자동으로 실행
- **조건부 실행**: 데이터가 도착했는지 확인 후 처리 시작
- **실행 기록**: 모든 실행 과정과 결과를 저장하고 분석
- **완료 시간 관리**: 작업이 예상 시간 내에 완료되는지 확인하고 지연 시 알림

### 4.3 모니터링 및 알림

#### 핵심 모니터링 지표

- **데이터 품질 지표**: 완전성(누락 없음), 정확성, 일관성, 적시성 등
- **시스템 성능 지표**: 처리 소요 시간, 컴퓨팅 자원 사용량, 오류 발생 비율
- **비즈니스 지표**: 데이터 양, 패턴 변화, 비정상 값 비율 등

#### 알림 설정 및 대응 프로세스

- **알림 방법**: Slack, 이메일 등을 통한 문제 상황 전파
- **담당자 지정**: 문제의 종류와 심각도에 따른 담당자 자동 지정
- **단계적 확대**: 지정 시간 내 응답이 없으면 상위 담당자에게 알림
- **문제 해결 기록**: 문제 발생부터 해결까지의 전체 과정 문서화

## 5. 확장성 및 향후 계획

### 데이터 규모 확장 대응 방안

- **자동 자원 확장**: 데이터 양에 따라 컴퓨팅 자원을 자동으로 늘리거나 줄임
- **데이터 분할 전략 개선**: 늘어나는 데이터에 맞게 효율적인 분할 방법 적용
- **저장 효율화**: 데이터 압축과 인덱싱으로 저장 공간 절약 및 검색 속도 향상

### 추가 데이터 소스 통합 가능성

- **표준화된 연결 과정**: 새로운 데이터 제공 업체를 쉽게 추가할 수 있는 절차 수립
- **데이터 구조 변화 관리**: 시간에 따라 변하는 데이터 구조에 대응하는 방법
- **다양한 형식 지원 확대**: API, 실시간 데이터 등 새로운 형태의 데이터 소스 통합 계획

### 실시간 처리 도입 검토

- **혼합 처리 방식**: 일괄 처리와 실시간 처리를 함께 사용하는 방식
- **이중 구조 아키텍처**: 느린 일괄 처리와 빠른 실시간 처리 레이어 조합
- **즉각적인 품질 관리**: 실시간으로 데이터 품질 문제를 감지하고 대응

## 6. 개인적인 소감 및 배운 점

### 파이프라인 구축과 데이터 품질의 중요성

처음에는 단순히 데이터를 A에서 B로 옮기는 파이프라인 구축이 주된 작업일 것이라 생각했다. 하지만 실제로는 데이터 품질 관리(DQA)가 훨씬 더 큰 비중을 차지했다. 데이터를 이동시키는 것은 코드 몇 줄로 해결되지만, 그 데이터가 신뢰할 수 있고 일관된 것인지 확인하는 과정이 더 많은 시간과 노력을 필요로 했다. 특히 업종별 특성을 고려한 검증 규칙을 만드는 것은 단순한 기술적 문제를 넘어서, 비즈니스에 대한 깊은 이해가 필요한 작업이었다.

### 확장성과 실용성 사이의 균형

초기에는 미래의 가능성까지 고려하며 설계하려 했다. 각 데이터 소스별로 어댑터 패턴을 적용해 코드 수정을 최소화하려 했지만, 이는 오히려 코드를 불필요하게 복잡하게 만들었다. 이를 통해 깨달은 점은, 현재 확실히 필요한 기능은 구현하되 불확실한 미래를 위한 과도한 설계는 피해야 한다는 것이다. 결국 실용성과 확장성 사이에서 적절한 균형을 찾는 것이 핵심이었다.

### 실무자와의 협업 경험

이 프로젝트는 많은 회의와 다양한 팀원들과의 소통이 필요했다. 완벽한 데이터는 현실에서 찾기 어렵고, 각 데이터 제공자마다 데이터에 대한 이해도와 관리 수준이 달랐다. 기술적인 해결책도 중요하지만, 효과적인 소통과 비즈니스 프로세스에 대한 이해가 성공적인 데이터 파이프라인 구축의 핵심이라는 것을 배웠다.

### 가격 최적화 모델과의 연계성

가격 최적화 시스템의 전체 구조를 아직 완벽히 이해하지는 못했지만, 이 프로젝트를 통해 점차 더 깊이 있게 파악할 수 있을 것 같다. 데이터 파이프라인은 결국 분석 모델의 기반이 되므로, 두 요소가 어떻게 유기적으로 연결되는지 이해하는 것이 다음 과제다. 데이터 품질이 모델의 정확도에 직접적인 영향을 미친다는 점을 인식하고, 파이프라인의 각 단계가 최종 목표에 어떻게 기여하는지 더 깊이 고민해볼 예정이다.

## 맺음말

데이터 파이프라인 구축 프로젝트를 통해 ETL과 데이터 품질 관리의 실제 적용 방법을 배울 수 있었다. 처음에는 생소했던 개념들이었지만, 프로젝트를 진행하면서 데이터 처리의 각 단계가 왜 중요한지, 어떤 문제들을 해결해야 하는지 이해하게 되었다. 특히 데이터 품질 관리가 전체 시스템의 신뢰성에 미치는 영향을 직접 경험하고 있다.

ETL과 데이터 품질 관리라는 개념이 처음에는 생소했지만, 이제는 조금씩 그 의미와 중요성을 체감하고 있다. 앞으로도 이 프로젝트를 통해 많은 것을 배울 수 있을 것 같다. 중요한 데이터를 다루는 시스템을 설계하고 구축한다는 책임감과 설렘을 가지고 계속해서 도전해 나가려 한다.

## 용어 설명

- **AWS S3(Simple Storage Service)**: 아마존에서 제공하는 클라우드 스토리지 서비스로, 무제한에 가까운 저장 공간과 높은 내구성을 제공한다.
- **Data Lake**: 원시 형태의 다양한 데이터를 저장하는 중앙 저장소로, 구조화/비구조화 데이터를 모두 보관할 수 있다.
- **Databricks**: Apache Spark 기반의 클라우드 기반 빅데이터 및 머신러닝 플랫폼으로, 데이터 분석, 처리, 모델링을 통합 환경에서 제공한다.
- **DQA(Data Quality Assurance)**: 데이터의 정확성, 완전성, 일관성 등을 보장하기 위한 품질 관리 활동이다.
- **ETL(Extract, Transform, Load)**: 데이터를 원천 시스템에서 추출하여, 변환 과정을 거친 후, 목적지 시스템에 적재하는 과정을 의미한다.
- **Lambda 아키텍처**: 배치 처리와 실시간 처리를 결합한 데이터 처리 아키텍처로, 지연 시간과 처리량 사이의 균형을 맞추는 방식이다.
- **Parquet**: 컬럼 기반 저장 형식으로, 데이터 압축 효율과 쿼리 성능이 뛰어난 오픈소스 파일 포맷이다.
- **PySpark**: Apache Spark의 Python API로, 대규모 데이터 처리를 위한 분산 컴퓨팅 프레임워크이다.
- **SLA(Service Level Agreement)**: 서비스 제공자와 사용자 간에 합의된 서비스 수준(가용성, 성능 등)을 명시한 계약이다.
- **어댑터 패턴(Adapter Pattern)**: 서로 다른 인터페이스를 가진 클래스들이 함께 작동할 수 있도록 중간에서 변환해주는 설계 패턴이다.
- **오버엔지니어링(Over-engineering)**: 실제 필요 이상으로 복잡하게 시스템을 설계하는 것으로, 유지보수 비용을 증가시킬 수 있다.
- **증분 처리(Incremental Processing)**: 전체 데이터가 아닌 새로 추가된 데이터만 처리하는 방식으로, 효율성을 높인다.
- **파티셔닝(Partitioning)**: 데이터를 특정 기준(날짜, 지역 등)으로 나누어 저장하는 기법으로, 쿼리 성능을 향상시킨다.
