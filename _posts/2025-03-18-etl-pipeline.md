---
layout: post
title: "ETL 파이프라인: Databricks와 PySpark를 활용한 데이터 처리 시스템"
date: 2025-03-17 12:00:00 +0900
categories: [개발, 데이터엔지니어링]
tags: [etl, databricks, pyspark, aws-s3, data-engineering]
mermaid: true
---

<style>
.mermaid {
  width: 100%;
  max-width: 1200px;
  margin: 40px auto;
  font-size: 18px;
  font-family: 'Arial', sans-serif;
  overflow: visible;
}
.mermaid .node rect, 
.mermaid .node circle, 
.mermaid .node ellipse, 
.mermaid .node polygon, 
.mermaid .node path {
  fill: #f5f9ff;
  stroke: #4a6da7;
  stroke-width: 1.5px;
}
.mermaid .node text {
  font-size: 18px;
  font-weight: 500;
}
.mermaid .edgeLabel {
  font-size: 16px;
  background-color: white;
  padding: 4px 8px;
  border-radius: 4px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}
.mermaid .cluster rect {
  fill: #f0f8ff;
  stroke: #4a6da7;
  stroke-width: 1px;
  rx: 8px;
  ry: 8px;
}
.mermaid .label {
  font-size: 20px;
  font-weight: bold;
}
.mermaid .timeline-event {
  font-size: 16px;
}
.mermaid .journey-section {
  font-size: 18px;
  font-weight: bold;
}
</style>

## 1. 개요

최근 수요 예측과 가격 최적화 모델을 위한 데이터 처리 시스템 개발을 담당하게 되었다. 초기에는 내부 팀과 외부 업체 1곳의 데이터만을 처리하는 소규모 시스템으로 시작하였으나, 향후 다수의 데이터 소스를 수용해야 할 것으로 예상되어 확장성 있는 시스템 설계가 필수적이었다.

현재 수집되는 데이터를 분석한 결과, 데이터 품질에 있어 여러 문제점이 발견되었다. 누락된 정보, ID 중복, 비정상적인 값과 미처리된 null 값 등이 주요 이슈였으며, 이러한 데이터 품질 문제 해결이 최우선 과제로 대두되었다.

시스템 설계 과정에서 상세한 설계 문서를 작성하고, Raw 데이터 통합을 위한 초기 프로토타입을 구현하였다. 이 과정에서 다양한 기술적 검토와 심도 있는 분석이 요구되었다. 현재 프로젝트가 진행 중이지만, 지금까지의 개발 경험과 인사이트를 공유하고자 했다.

### 핵심 요구사항

- **확장성**: 데이터 제공 업체가 늘어나도 유연하게 대응할 수 있는 구조
- **데이터 품질**: 믿을 수 있고 일관된 데이터 제공
- **자동화**: 사람이 직접 개입하는 과정을 최소화한 데이터 처리
- **표준화**: 다양한 형식의 데이터를 일관된 형태로 변환

### 기대효과

데이터 처리 시스템을 구축함으로써 다음과 같은 효과를 기대할 수 있다:

- **수동 데이터 수집 과정 자동화**: 작업 효율 향상 및 사람의 실수 감소
- **일관된 데이터 처리 규칙 적용**: 데이터 품질 향상
- **체계적인 데이터 검증**: 신뢰할 수 있는 데이터 확보
- **재현 가능한 데이터 처리**: 투명성 및 추적 가능성 확보
- **모델링 시간 단축**: 데이터 준비 시간을 줄여 실제 분석 모델 개발에 집중

## 2. 아키텍처 설계

### 2.1 핵심 구성요소

- **AWS S3 (데이터 저장소)**: 원본 데이터와 처리된 데이터를 저장하는 공간이다. 대용량 데이터를 안정적으로 저장하고 필요할 때 쉽게 접근할 수 있다.

- **Databricks (처리 환경)**: 대규모 데이터 처리와 머신러닝을 위한 플랫폼으로, 팀원들이 함께 코드를 작성하고 실행할 수 있는 환경을 제공한다.

- **PySpark (데이터 처리 도구)**: 여러 컴퓨터에 데이터 처리 작업을 분산하여 대용량 데이터를 빠르게 처리할 수 있게 해주는 Python 라이브러리이다.

- **Parquet (데이터 저장 형식)**: 데이터를 효율적으로 압축하고 빠르게 읽을 수 있는 파일 형식으로, 데이터의 구조 정보도 함께 저장하여 일관성을 유지한다.

<div class="mermaid">
flowchart LR
    classDef storage fill:#f9f7ed,stroke:#d4b483,stroke-width:2px
    classDef process fill:#e1f0fa,stroke:#4a6fa5,stroke-width:2px
    classDef data fill:#ebf5ee,stroke:#58a4b0,stroke-width:2px
    
    S3["AWS S3<br><small>데이터 저장소</small>"]:::storage
    DB["Databricks<br><small>처리 환경</small>"]:::process
    PS["PySpark<br><small>데이터 처리</small>"]:::process
    ML["ML 모델<br><small>가격 최적화</small>"]:::data
    
    S3 --> |"원본 데이터"| DB
    DB --> |"처리"| PS
    PS --> |"변환된 데이터"| S3
    S3 --> |"학습 데이터"| ML
    
    style S3 font-size:18px,font-weight:bold
    style DB font-size:18px,font-weight:bold
    style PS font-size:18px,font-weight:bold
    style ML font-size:18px,font-weight:bold
</div>

*ETL 파이프라인의 주요 구성 요소(AWS S3, Databricks, PySpark, ML 모델) 간의 데이터 흐름과 각 구성 요소별 역할의 시각화.*

### 2.2 데이터 흐름도

<div class="mermaid">
timeline
    title ETL 파이프라인 처리 흐름
    section 데이터 수집
        원본 데이터 도착 : 업체 파일 수신
        초기 점검 : 파일 형식 및 구조 검증
    section 데이터 처리
        변환 : 표준 형식으로 변환
        검증 : 품질 검사 적용
        보강 : 파생 속성 추가
    section 데이터 저장
        저장소 적재 : Parquet 파일로 저장
        분할 : 날짜 및 출처별 구성
    section 데이터 활용
        분석 : 분석 쿼리 지원
        ML 모델 : 가격 최적화 모델 학습
</div>

*ETL 과정의 주요 단계(데이터 수집, 처리, 저장, 활용)의 시간 순서별 표현.*

데이터 흐름은 다음과 같은 단계로 이루어진다:

1. **데이터 수집**: 각 업체의 데이터가 매일 일정 시간에 AWS S3에 저장된다.
2. **데이터 불러오기**: 저장된 데이터를 Databricks 환경으로 가져온다.
3. **데이터 처리 및 변환**: PySpark를 사용해 데이터의 품질을 검증하고, 일관된 형식으로 변환한다.
4. **결과 저장 및 활용**: 처리된 데이터를 Parquet 형식으로 저장하고, 이를 가격 최적화 모델과 다양한 분석에 활용한다.

## 3. 데이터 처리 파이프라인

### 3.1 데이터 수집 전략

#### 소스 데이터 종류 및 형식

외부 업체에서 제공하는 데이터는 다음과 같은 형식으로 제공된다:

- **구조화된 데이터**: CSV, JSON 같은 일반적인 형식의 거래 데이터
- **파일 전송 방식**: 매일 정해진 시간에 안전한 파일 전송 방식으로 전달
- **데이터 종류**: 상품 가격, 재고 정보, 예약 현황, 할인 정보 등

#### 수집 주기 및 방법

- **수집 주기**: 하루에 한 번, 주로 새벽 시간대에 처리
- **업체별 데이터 변환기**: 각 업체의 데이터 형식에 맞는 맞춤형 변환 프로그램 개발
- **메타데이터 관리**: 파일 도착 시간, 처리 상태, 데이터 품질 점수 등의 부가 정보 관리

### 3.2 데이터 검증 프로세스

데이터 품질을 보장하기 위해 4단계로 구성된 체계적인 검증 과정을 구현한다:

<div class="mermaid">
stateDiagram-v2
    [*] --> 기본검증
    
    state 기본검증 {
        빈값확인
        데이터유형확인
        값범위확인
        형식확인
    }
    
    기본검증 --> 업종특화검증
    
    state 업종특화검증 {
        가격합리성확인
        재고관리검증
        예약충돌확인
        시즌별가격검증
        할인규칙확인
        날짜패턴확인
    }
    
    업종특화검증 --> 이상패턴감지
    
    state 이상패턴감지 {
        통계적이상감지
        시간패턴분석
        급격한변동감지
        계절패턴적용
    }
    
    이상패턴감지 --> 문제대응
    
    state 문제대응 {
        심각도분류
        업체알림
        내부팀알림
        자동수정적용
        수동검토요청
        문제추적관리
    }
    
    문제대응 --> [*]
</div>

*데이터 검증 프로세스의 4단계(기본 검증, 업종 특화 검증, 이상 패턴 감지, 문제 대응)와 각 단계별 세부 작업의 도식화.*

#### 검증 아키텍처 설계 시 고려사항

1. **검증 시점 결정**
   - **선택: 단계별 검증 방식**
   - **고려했던 대안들**:
     - **일괄 검증**: 모든 검증을 한 번에 수행
     - 기각 이유: 오류 발생 시 전체 파이프라인 재실행 필요
     - **실시간 검증**: 데이터 유입 시점에 즉시 검증
     - 기각 이유: 시스템 복잡도 증가, 처리 지연 발생

2. **검증 실패 대응 전략**
   - **선택: 심각도 기반 차등 대응**
   - **고려했던 대안들**:
     - **모든 오류 중단**: 완벽한 데이터 품질 보장
     - 기각 이유: 과도한 운영 중단, 비즈니스 연속성 저해
     - **모든 오류 허용**: 처리 속도 최적화
     - 기각 이유: 데이터 품질 보장 불가

### 3.3 데이터 변환 및 처리

#### PySpark 변환 로직

주요 데이터 처리 로직의 예시이다:

```python
# 주중/주말 가격 패턴 검증 예시
def validate_pricing_pattern(df):
    # 요일 정보 추출
    df['day_of_week'] = df['date'].dt.dayofweek
    
    # 주중/주말 구분
    weekday_prices = df[df['day_of_week'].isin([0,1,2,3,4])['price']
    weekend_prices = df[df['day_of_week'].isin([5,6])]['price']
    
    # 평균 가격 계산
    avg_weekday = weekday_prices.mean()
    avg_weekend = weekend_prices.mean()
    
    # 일반적인 패턴 검증
    if avg_weekend < avg_weekday * 0.8:
        raise_alert(
            severity="High",
            message=f"비정상 가격 패턴 감지: 주말 평균 {avg_weekend}이 주중 평균 {avg_weekday}의 80% 미만",
            metrics={"weekend_avg": avg_weekend, "weekday_avg": avg_weekday, "ratio": avg_weekend/avg_weekday}
        )
```

#### 처리 아키텍처 선정 과정

1. **데이터 처리 방식**
   - **선택: 배치 처리 + 증분 업데이트**
   - **고려했던 대안들**:
     - **순수 스트리밍**: 실시간 처리
     - 기각 이유: 구현 복잡도 증가, 비용 효율성 저하
     - **전체 배치**: 매일 전체 데이터 재처리
     - 기각 이유: 처리 시간 증가, 리소스 낭비

#### 성능 최적화 방안

이러한 의사결정 과정을 거치면서 얻은 주요 교훈은 다음과 같다:
- 현재 요구사항과 미래 확장성 사이의 균형
- 운영 복잡도와 기능의 트레이드오프
- 팀의 기술적 역량과 학습 곡선 고려

## 4. 개발 및 운영 전략

### 4.1 개발 환경

#### Databricks 노트북 활용 방안

- **통합 개발 환경**: 코드, 설명, 시각화를 한 곳에서 작성하고 확인
- **변수 설정**: 실행 시 필요한 값을 쉽게 변경할 수 있는 방식으로 구성
- **함께 작업**: 여러 팀원이 동시에 같은 문서를 편집하고 의견 교환
- **변경 이력 관리**: 노트북의 모든 변경사항을 기록하고 필요시 이전 버전으로 복원

#### 버전 관리 및 협업

- **코드 관리 시스템 연동**: Databricks와 Git을 연결하여 코드 변경 이력 관리
- **환경 분리**: 개발, 테스트, 운영 환경을 분리하여 안정성 확보
- **코드 검토**: 변경 사항에 대한 팀원 검토 과정 도입
- **상세한 설명**: 코드 내 주석과 마크다운을 활용한 풍부한 문서화

### 4.2 ETL 파이프라인 운영 프로세스

<div class="mermaid">
flowchart LR
    classDef extract fill:#f9d5e5,stroke:#d64161,stroke-width:2px
    classDef transform fill:#dcf2e9,stroke:#20b2aa,stroke-width:2px
    classDef load fill:#e6e6fa,stroke:#6a5acd,stroke-width:2px
    
    subgraph 추출단계["추출(Extract)"]
        E1["데이터 도착 확인<br><small>파일 존재 여부 체크</small><br><small>기본 메타데이터 확인</small>"]:::extract
        E2["데이터 수집<br><small>Raw 데이터 적재</small><br><small>수집 이력 기록</small>"]:::extract
    end
    
    subgraph 변환단계["변환(Transform)"]
        T1["기본 검증<br><small>필수 컬럼 확인</small><br><small>데이터 타입 검증</small>"]:::transform
        T2["업종별 검증<br><small>업종 코드 검증</small><br><small>필수값 존재 확인</small>"]:::transform
        T3["표준화<br><small>공통 스키마 적용</small><br><small>코드값 정규화</small>"]:::transform
    end
    
    subgraph 적재단계["적재(Load)"]
        L1["데이터 적재<br><small>적재 위치 확인</small><br><small>적재 이력 기록</small>"]:::load
        L2["적재 후 검증<br><small>데이터 건수 확인</small><br><small>무결성 체크</small>"]:::load
    end
    
    E1 --> E2 --> T1 --> T2 --> T3 --> L1 --> L2
    
    style 추출단계 fill:#fef6f8,stroke:#d64161,stroke-width:2px
    style 변환단계 fill:#f0f9f6,stroke:#20b2aa,stroke-width:2px
    style 적재단계 fill:#f5f5fd,stroke:#6a5acd,stroke-width:2px
</div>

*ETL 파이프라인의 주요 단계와 세부 작업의 시각화. 추출(분홍색), 변환(민트색), 적재(보라색) 단계 구분과 세부 작업 내용의 표현.*

#### ETL 파이프라인 운영 단계 설명

ETL 파이프라인의 운영은 크게 세 단계로 나뉘며, 각 단계별로 다음과 같은 주요 작업들이 있다:
1. **추출(Extract) 단계**:
   - **데이터 도착 확인**: 데이터 처리의 첫 단계로, 외부 업체로부터 데이터가 정해진 시간에 도착했는지 확인한다. 데이터가 늦게 도착하면 후속 프로세스에 영향을 미치므로, 분산된 raw 데이터를 테이블별로 통합하고 도착 여부를 검증한다. 이는 ETL 시스템이 자동으로 수행하여 실시간 모니터링이 가능하다.
   - **데이터 수집**: 안정적인 데이터 수집을 위해 S3 버킷과 Databricks 환경의 연결 설정, IAM 역할과 권한 설정 등의 자동화된 프로세스를 사전에 구축한다. 이를 통해 데이터 유실 없이 S3의 데이터를 Databricks로 안전하게 가져올 수 있다.
2. **변환(Transform) 단계**:
   - **기본 검증**: 잘못된 데이터가 후속 프로세스에 영향을 미치는 것을 방지하기 위해 데이터 형식, 범위, 필수값 존재 여부 등을 확인한다. 팀이 정의한 검증 기준에 따라 자동화된 검증 프로세스로 수행되어 일관된 품질을 보장한다.
   - **파트너사별 검증**: 각 파트너사마다 데이터 특성이 다르므로, 파트너사별 담당 부서와 협의하여 정의된 비즈니스 규칙에 맞게 데이터를 검증한다. 파트너사와 도메인 전문가의 검토를 거쳐 확정된 규칙을 기반으로 자동화된 검증이 이루어져 데이터의 비즈니스적 정확성을 보장한다.
   - **표준화**: 여러 파트너사에서 오는 데이터 형식이 다르면 통합 분석이 어려우므로, 팀의 데이터 표준화 정책에 따라 일관된 형태로 변환한다. 이 단계에서 모든 파트너사의 데이터 스키마가 통일되어 후속 분석 작업의 효율성이 높아진다.

3. **적재(Load) 단계**:
   - **데이터 적재**: 분석 성능과 저장 효율성을 위해 변환된 데이터를 Parquet 형식으로 S3에 저장한다. Parquet은 컬럼 기반 저장 방식으로 데이터 압축률이 높고 특정 컬럼만 조회할 때 성능이 우수하며, 스키마가 자동으로 보존되어 데이터 일관성 유지에도 도움이 된다.
   - **적재 후 검증**: 적재 과정에서 데이터 손실이나 변형이 발생할 수 있고, 스토리지 문제로 인한 부분 저장 등의 이슈를 잡아내기 위해 최종 검증이 필요하다. 원본 데이터와의 레코드 수 비교, 주요 지표 값 검증 등을 자동화된 검증 프로세스를 통해 수행하여 데이터의 완전성을 보장한다.

이러한 단계적 접근을 통해 데이터 처리의 신뢰성과 효율성을 확보할 수 있다. 각 단계는 자동화된 시스템을 통해 처리되며, 문제 발생 시 즉각적인 알림과 로깅을 통해 신속한 대응이 가능하도록 설계되어 있다.

### 4.3 개발 로드맵

<div class="mermaid">
gantt
    title ETL 파이프라인 개발 로드맵
    dateFormat  YYYY-MM-DD
    axisFormat %y-%m
    
    section 완료된 작업
    데이터 수집 자동화      :done, task1, 2025-02-01, 2025-03-15
    기본 ETL 파이프라인 구축 :done, task2, 2025-02-15, 2025-04-15
    
    section 진행중
    데이터 품질 관리 체계화  :active, task3, 2025-04-01, 2025-05-31
    
    section 단기 계획 (6개월)
    추가 데이터 소스 통합    :task4, 2025-06-01, 90d
    성능 최적화            :task5, after task4, 60d
    모니터링 개선           :task6, 2025-09-01, 90d
    
    section 장기 계획
    ML 파이프라인 연동      :task8, 2025-10-01, 90d
</div>

*ETL 파이프라인 개발의 타임라인 시각화. 2025년 2월부터 시작된 프로젝트의 완료된 작업(회색), 진행 중인 작업(주황색), 단기 계획 및 장기 계획의 시간순 정렬.*

## 5. 확장성 및 향후 계획

### 데이터 규모 확장 대응 방안

- **자동 자원 확장**: 데이터 양에 따라 컴퓨팅 자원을 자동으로 늘리거나 줄임
- **데이터 분할 전략 개선**: 늘어나는 데이터에 맞게 효율적인 분할 방법 적용
- **저장 효율화**: 데이터 압축과 인덱싱으로 저장 공간 절약 및 검색 속도 향상

### 추가 데이터 소스 통합 가능성

- **표준화된 연결 과정**: 새로운 데이터 제공 업체를 쉽게 추가할 수 있는 절차 수립
- **데이터 구조 변화 관리**: 시간에 따라 변하는 데이터 구조에 대응하는 방법
- **다양한 형식 지원 확대**: API, 실시간 데이터 등 새로운 형태의 데이터 소스 통합 계획

## 6. 개인적인 소감 및 배운 점

### 파이프라인 구축과 데이터 품질의 중요성

파이프라인을 만드는 것이 주요 작업일 것이라고 생각했는데, 실제로는 데이터 품질 관리(DQA)가 훨씬 더 큰 작업이었다. 코드 몇 줄로 데이터를 A에서 B로 옮기는 것은 상대적으로 쉽지만, 그 데이터가 신뢰할 수 있고 일관된 것인지 확인하는 과정에 더 많은 노력이 필요했다. 특히 업종에 대한 지식을 활용한 검증 규칙을 만드는 것은 단순한 기술적 문제를 넘어, 비즈니스 규칙에 대한 깊은 이해가 필요한 작업이었다.

### 확장성과 과도한 설계 사이의 균형

아직 추가되지도 않은 데이터 소스들을 미리 고려하면서 설계하는 것이 생각보다 어려웠다. 코드를 나중에 적게 수정할 수 있도록 만들려고 했는데, 오히려 각 데이터 출처별로 맞춤형 변환기를 만드는 방식을 선택하니 코드가 계속 늘어나는 것 같았다. 이런 접근법에는 장단점이 있다고 느꼈다. 확실히 추가될 요소라면 미리 구현하되, 불확실한 미래를 위해 너무 복잡한 설계를 하는 것은 과감히 포기하는 것도 필요하다는 교훈을 얻었다. 실용성과 확장성 사이에서 적절한 균형을 찾는 것이 매우 중요하다는 점을 배웠다.

### 실무자 협업의 중요성

이 프로젝트를 설계하고 기획하는 데 많은 회의와 다양한 팀원들과의 소통이 필요했다. 세상에 완벽한 데이터는 많지 않고, 각 데이터 제공자마다 데이터에 대한 이해와 관리 수준이 다르다는 점을 경험했다. 기술적인 해결책만으로는 충분하지 않으며, 효과적인 소통과 비즈니스 과정에 대한 이해가 성공적인 데이터 파이프라인 구축의 핵심이라는 점을 배웠다.

### 가격 최적화 모델에 대한 이해 확장

ETL 파이프라인을 구축하면서 가장 흥미로웠던 점은 데이터를 통해 시장 경제의 작동 방식을 들여다볼 수 있다는 것이었다. 수많은 변수들이 실시간으로 가격에 영향을 미치는 것을 보면서, 대학에서 배웠던 경제학 이론들이 실제로 어떻게 동작하는지 목격하는 것 같았다. 특히 경쟁사의 가격 변동이 수요에 미치는 영향이나, 계절성과 같은 시장 패턴이 데이터로 명확하게 드러나는 것을 보며 놀라웠다. 기술을 통해 시장의 숨겨진 패턴을 발견하고 이해하는 과정이 마치 퍼즐을 맞추는 것처럼 즐거웠다. 이런 경험은 내가 단순한 개발자가 아닌, 비즈니스와 기술의 접점에서 일하는 엔지니어라는 것을 실감하게 해주었다.

## 용어 정리

- **Apache Airflow**: 데이터 파이프라인의 프로그래밍, 스케줄링 및 모니터링을 위한 플랫폼
- **Batch Processing**: 일정량의 데이터를 모아서 한 번에 처리하는 방식
- **Data Pipeline**: 데이터의 수집, 처리, 저장, 분석 등 일련의 과정을 자동화한 시스템
- **Data Quality Assurance(DQA)**: 데이터의 정확성, 일관성, 신뢰성을 보장하기 위한 프로세스
- **ETL(Extract, Transform, Load)**: 데이터를 추출, 변환, 적재하는 일련의 데이터 처리 과정
- **Incremental Update**: 전체 데이터가 아닌 변경된 부분만 처리하는 방식
- **Layer Architecture**: 시스템을 논리적 계층으로 분리하여 설계하는 방식
- **Parquet**: 컬럼 기반의 데이터 저장 형식으로, 높은 압축률과 빠른 쿼리 성능을 제공
- **PySpark**: Apache Spark의 Python API로, 대규모 데이터 처리를 위한 분산 컴퓨팅 프레임워크
- **Real-time Processing**: 데이터가 생성되는 즉시 처리하는 방식
- **Schema Evolution**: 시간이 지남에 따라 데이터 구조가 변화하는 것을 관리하는 방법
- **Streaming**: 실시간으로 연속적인 데이터를 처리하는 방식