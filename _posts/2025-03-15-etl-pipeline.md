---
layout: post
title: "파트너사 확장에 따른 데이터 수집 파이프라인 설계 사례"
date: 2025-05-03 10:00:00 +0900
categories: Data Engineering
tags: [Data Engineering, Data Pipeline, Databricks, Workflow, Ingestion, Distributed Processing]
mermaid: true
---

# 파트너사 확장에 따른 데이터 수집 파이프라인 설계 사례

## 1. 배경

기존에는 Airflow 기반의 전용 데이터 수집 파이프라인을 운영하고 있었습니다.  
해당 파이프라인은 특정 파트너사의 `.tar.gz` 압축 파일을 수신하여 내부의 `.csv` 테이블 데이터를 정제 및 병합한 후 저장하는 구조였습니다.  
파일 구조와 테이블 스키마가 고정되어 있었기 때문에 안정적으로 운영할 수 있었습니다.

그러나 파트너사 수가 증가하면서 다양한 포맷의 데이터가 유입되었고, 기존 구조로는 확장에 한계가 발생하였습니다.  
이에 따라 Databricks 기반의 공용 데이터 수집 파이프라인을 신규로 설계 및 구축하는 과제를 수행하게 되었습니다.

```mermaid
graph TD
    A[Airflow 기반 전용 파이프라인] --> B[파트너사 A]
    A --> C[데이터 저장소 A]
    D[Databricks 공용 파이프라인] --> E[파트너사 A~N]
    D --> F[파트너사별 저장소 구조]
    A -. 유지 -.-> D

2. 기획 및 요구사항 정리

공용 파이프라인을 설계하기 위해 다음과 같은 요구사항을 정리하였습니다.
	•	파트너사 간 데이터 분리가 반드시 보장되어야 했습니다.
	•	테이블 스키마가 파트너사마다 상이하며, 일부 테이블은 컬럼 수나 순서가 달라졌습니다.
	•	처리 흐름의 병렬성과 유연성이 확보되어야 했습니다.
	•	실패한 작업만 부분 재실행할 수 있도록 구조가 나뉘어야 했습니다.

이를 바탕으로 다음과 같은 설계 방향을 도출하였습니다.

graph LR
    A[요구사항 도출]
    B[데이터 분리]
    C[스키마 다양성 대응]
    D[작업 병렬 처리]
    E[Task 재실행 가능성 확보]

    A --> B
    A --> C
    A --> D
    A --> E

3. 구현

3.1 Workflow 구성

Databricks Workflow를 기반으로 전체 배치를 Task 단위로 분리하였습니다.
각 Task는 독립적으로 실행될 수 있도록 구성하였으며, 병렬 처리를 적용하여 전체 처리 속도를 개선하였습니다.

graph TD
    A[pre_set_date] --> B[wait_for_all_sources]
    B --> C[batch_extract]
    C --> D[merge_table_1]
    C --> E[merge_table_2]
    C --> F[merge_table_3]
    D --> G[merge_summary]
    E --> G
    F --> G
    G --> H[post_merge_check]

3.2 기술 구성 요소
	•	.tar.gz 압축 해제는 Python의 tarfile을 활용하여 수행하였습니다.
	•	S3 경로는 파트너사명 및 날짜 기반으로 디렉토리를 구성하였습니다.
	•	각 테이블은 JSON 기반 스키마 정보를 참조하여 병합 작업을 수행하였습니다.
	•	Databricks Workflow를 통해 작업 간 의존성을 정의하고, Task 단위 재시도를 지원하였습니다.

graph LR
    subgraph "파일 처리"
        A[압축 해제]
        B[S3 업로드]
    end
    subgraph "병합 처리"
        C[스키마 로딩]
        D[병합 수행]
    end
    A --> B --> C --> D

4. 결과 및 회고

4.1 주요 성과
	•	병렬 처리 도입으로 전체 처리 시간이 약 40~60% 감소하였습니다.
	•	Task 단위 재시작이 가능해져 장애 복구가 간소화되었습니다.
	•	신규 파트너사 추가 시 구조 전체를 재작성하지 않고도 대응이 가능해졌습니다.
	•	로그 및 알림 체계가 개선되어 운영 가시성이 향상되었습니다.

graph TD
    A[처리 시간] -->|↓ 60%| B[병렬화]
    C[복구 속도] -->|↑| D[Task 단위 재실행]
    E[구조 재사용] -->|↑| F[파트너사 추가 대응]

4.2 회고

기존에 안정적으로 운영되던 파이프라인 구조는 특정 조건에 최적화된 형태였습니다.
처음에는 기존 코드를 복사하여 확장하면 되지 않을까 생각했지만, 파트너사 수 증가와 요구 조건 다양화에 따라 이 접근은 금세 한계에 도달하였습니다.

이번 개선을 통해 작동하는 구조와 확장 가능한 구조는 다르다는 사실을 체감하게 되었으며,
장기적인 운영을 고려한 설계의 중요성을 다시 한 번 인식할 수 있었습니다.

⸻

5. 마무리

이번 프로젝트는 기존 시스템을 유지하면서, 새로운 요구사항에 대응하기 위해 Databricks 기반으로 공용 ingestion 파이프라인을 처음부터 새롭게 설계한 사례였습니다.
설계와 구현 과정에서 다양한 시도를 했으며, 구조적 확장성과 운영 효율성을 동시에 확보할 수 있었습니다.

향후에는 해당 파이프라인을 기반으로 Transformation 및 DQA 체계로 확장하여, 전체 데이터 처리 흐름을 통합적으로 관리할 수 있는 구조로 발전시킬 예정입니다.
